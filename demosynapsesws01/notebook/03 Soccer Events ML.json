{
	"name": "03 Soccer Events ML",
	"properties": {
		"folder": {
			"name": "Europe Soccer Events"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SoccerEvents",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/f6c0b928-7078-4332-a106-e7a9c5253249/resourceGroups/demo-synapse-rg/providers/Microsoft.Synapse/workspaces/demosynapsesws01/bigDataPools/SoccerEvents",
				"name": "SoccerEvents",
				"type": "Spark",
				"endpoint": "https://demosynapsesws01.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SoccerEvents",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# European Soccer Events - Azure Synapse Analytics Demo\r\n",
					"\r\n",
					"#### Project URL\r\n",
					"- <https://github.com/aessing/demo-azuresynapse>\r\n",
					"\r\n",
					"#### Developer\r\n",
					"Andre Essing\r\n",
					"- <https://www.andre-essing.de/>\r\n",
					"- <https://github.com/aessing>\r\n",
					"- <https://twitter.com/aessing>\r\n",
					"- <https://www.linkedin.com/in/aessing/>\r\n",
					"\r\n",
					"> THIS CODE AND INFORMATION ARE PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY AND/OR FITNESS FOR A PARTICULAR PURPOSE.\r\n",
					"\r\n",
					"---"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# European Soccer Events Analysis - Machine Learning\r\n",
					"\r\n",
					"As we saw, doing descriptive analysis on big data (like above) has been made super easy with Spark SQL and Databricks. But what if you’re a data scientist who’s looking at the same data to find combinations of on-field playing conditions that lead to “goals”?\r\n",
					"\r\n",
					"We’ll now create a third notebook from that perspective, and see how one could fit a [Gradient-boosted tree](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#gradient-boosted-tree-classifier) classifier Spark ML model on the game events training dataset. In this case, our binary classification label will be field “is_goal”, and we’ll use a mix of categorical features like “event_type_str”, “event_team”, “shot_place_str”, “location_str”, “assist_method_str”, “situation_str” and “country_code”.\r\n",
					"\r\n",
					"First, we need to do the necessary imports from Spark ML:"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Load data from Data Lake and create a TempView"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"gameEventsDf = spark.read.load('abfss://europesoccer@demosynapsedls01.dfs.core.windows.net/curated/game_events.parquet', format='parquet')\r\n",
					"gameEventsDf.createOrReplaceTempView('gameEvents')"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Explore data to establish features\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"SELECT * \r\n",
					"  FROM gameEvents\r\n",
					" LIMIT 100"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					" ## Create dataset for model training and prediction"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"gameEventsDf = spark.sql(\"select event_type_str, event_team, shot_place_str, location_str, assist_method_str, situation_str, country_code, is_goal from gameEvents\")\r\n",
					"\r\n",
					"display(gameEventsDf.limit(100))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Necessary imports for Spark ML pipeline"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from pyspark.ml import Pipeline\r\n",
					"from pyspark.ml.classification import GBTClassifier\r\n",
					"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\r\n",
					"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Convert our categorical feature columns to a single binary vector\r\n",
					"\r\n",
					"Then the following four-step process is required to convert our categorical feature columns to a single binary vector:\r\n",
					"\r\n",
					"- Convert string features to indices using StringIndexer\r\n",
					"- Transform feature indices to binary vectors using OneHotEncoder\r\n",
					"- Assemble different binary vector columns into a single vector using VectorAssembler"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Create a list of categorical features\r\n",
					"categFeatures = [\"event_type_str\", \"event_team\", \"shot_place_str\", \"location_str\", \"assist_method_str\", \"situation_str\", \"country_code\"]\r\n",
					"\r\n",
					"# Encode categorical string cols to label indices\r\n",
					"stringIndexers = [StringIndexer().setInputCol(baseFeature).setOutputCol(baseFeature + \"_idx\") for baseFeature in categFeatures]\r\n",
					"\r\n",
					"# Convert categorical label indices to binary vectors\r\n",
					"encoders = [OneHotEncoder().setInputCol(baseFeature + \"_idx\").setOutputCol(baseFeature + \"_vec\") for baseFeature in categFeatures]\r\n",
					"\r\n",
					"# Combine all columns into a single feature vector\r\n",
					"featureAssembler = VectorAssembler()\r\n",
					"featureAssembler.setInputCols([baseFeature + \"_vec\" for baseFeature in categFeatures])\r\n",
					"featureAssembler.setOutputCol(\"features\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create Spark ML pipeline using a GBT classifier\r\n",
					"\r\n",
					"Finally, we’ll create a Spark ML Pipeline using the above transformers and the GBT classifier. We’ll divide the game events data into training and test datasets, and fit the pipeline to the former."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"gbtClassifier = GBTClassifier(labelCol=\"is_goal\", featuresCol=\"features\", maxDepth=5, maxIter=20)\r\n",
					"\r\n",
					"pipelineStages = stringIndexers + encoders + [featureAssembler, gbtClassifier]\r\n",
					"pipeline = Pipeline(stages=pipelineStages)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Split dataset into training/test, and create a model from training data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"(trainingData, testData) = gameEventsDf.randomSplit([0.75, 0.25])\r\n",
					"model = pipeline.fit(trainingData)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Validate the model on test data, display predictions\r\n",
					"\r\n",
					"Now we can validate our classification model by running inference on the test dataset. We could compare the predicted label with actual label one by one, but that could be a painful process for lots of test data. For scalable model evaluation in this case, we can use BinaryClassificationEvaluator with area under ROC metric. One could also use the Precision-Recall Curve as an evaluation metric. If it was a multi-classification problem, we could’ve used the MulticlassClassificationEvaluator."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"predictions = model.transform(testData)\r\n",
					"display(predictions.select(\"prediction\", \"is_goal\", \"features\"))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Evaluate the model using areaUnderROC metric"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"evaluator = BinaryClassificationEvaluator(\r\n",
					"    labelCol=\"is_goal\", rawPredictionCol=\"prediction\")\r\n",
					"evaluator.evaluate(predictions)"
				],
				"execution_count": null
			}
		]
	}
}